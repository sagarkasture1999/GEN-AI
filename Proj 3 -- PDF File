import importlib
import subprocess
import sys

# -----------------------------
# Helper: Install Missing Libraries
# -----------------------------
def install_if_missing(package, import_name=None):
    if import_name is None:
        import_name = package
    try:
        importlib.import_module(import_name)
        print(f"âœ… {package} already installed")
    except ImportError:
        print(f"ðŸ“¦ Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Required libraries
install_if_missing("PyPDF2")
install_if_missing("langchain")
install_if_missing("sentence-transformers", "sentence_transformers")
install_if_missing("faiss-cpu", "faiss")
install_if_missing("transformers")
install_if_missing("torch")
install_if_missing("numpy")

# -----------------------------
# Step 1: Read PDF
# -----------------------------
import PyPDF2

def read_pdf(file_path):
    text = ""
    with open(file_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            if page.extract_text():
                text += page.extract_text() + "\n"
    return text

pdf_path = r"E:\Downloads\GEN AI\Sagar_resume.PDF"
pdf_text = read_pdf(pdf_path)
print("\nâœ… Step 1: PDF loaded. Preview:\n", pdf_text[:300])

# -----------------------------
# Step 2: Split into Chunks
# -----------------------------
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = text_splitter.split_text(pdf_text)
print(f"\nâœ… Step 2: Split into {len(chunks)} chunks. First chunk:\n", chunks[0])

# -----------------------------
# Step 3: Embeddings
# -----------------------------
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(chunks, show_progress_bar=True)
print("\nâœ… Step 3: Embeddings created with shape:", embeddings.shape)

# -----------------------------
# Step 4: Store in FAISS
# -----------------------------
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))
print("âœ… Step 4: Stored in FAISS index")

# -----------------------------
# Step 5 & 6: Chatbot Loop
# -----------------------------
from transformers import pipeline
generator = pipeline("text2text-generation", model="google/flan-t5-large")

print("\nðŸ¤– Chatbot ready! Ask me questions about your PDF. Type 'bye' to exit.\n")

while True:
    query = input("You: ")
    if query.lower() == "bye":
        print("ðŸ¤– Chatbot: Goodbye ðŸ‘‹")
        break

    # Retrieve top 3 chunks
    query_vector = embedding_model.encode([query])
    D, I = index.search(np.array(query_vector), k=3)
    context = " ".join([chunks[i] for i in I[0]])

    # Generate answer
    prompt = f"Based only on this context: {context}\nAnswer the question: {query}"
    response = generator(prompt, max_new_tokens=200)[0]["generated_text"]

    print("ðŸ¤– Chatbot:", response)
